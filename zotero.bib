
@article{dietterich_solving_1997,
	title = {Solving the multiple instance problem with axis-parallel rectangles},
	volume = {89},
	issn = {0004-3702},
	doi = {10.1016/S0004-3702(96)00034-3},
	abstract = {The multiple instance problem arises in tasks where the training examples are ambiguous: a single example object may have many alternative feature vectors (instances) that describe it, and yet only one of those feature vectors may be responsible for the observed classification of the object. This paper describes and compares three kinds of algorithms that learn axis-parallel rectangles to solve the multiple instance problem. Algorithms that ignore the multiple instance problem perform very poorly. An algorithm that directly confronts the multiple instance problem (by attempting to identify which feature vectors are responsible for the observed classifications) performs best, giving 89\% correct predictions on a musk odor prediction task. The paper also illustrates the use of artificial data to debug and compare these algorithms.},
	number = {1},
	urldate = {2017-05-31},
	journal = {Artificial Intelligence},
	author = {Dietterich, Thomas G. and Lathrop, Richard H. and Lozano-Pérez, Tomás},
	month = jan,
	year = {1997},
	keywords = {Drug design, Machine learning, Structure-activity relationships},
	pages = {31--71}
}

@techreport{arthur_k-means++:_2006,
	type = {Technical {Report}},
	title = {k-means++: {The} {Advantages} of {Careful} {Seeding}},
	shorttitle = {k-means++},
	url = {http://ilpubs.stanford.edu:8090/778/},
	abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a simple, randomized seeding technique, we obtain an algorithm that is \$O({\textbackslash}log k)\$-competitive with the optimal clustering. Experiments show our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
	number = {2006-13},
	institution = {Stanford InfoLab},
	author = {Arthur, David and Vassilvitskii, Sergei},
	month = jun,
	year = {2006},
	keywords = {clustering, k-means, seeding},
	pages = {1027--1035}
}

@inproceedings{macqueen_methods_1967,
	address = {Berkeley, California},
	title = {Some methods for classification and analysis of multivariate observations},
	volume = {1},
	abstract = {The main purpose of this paper is to describe a process for partitioning an N-dimensional population into k sets on the basis of a sample. The process, which is called 'k-means, ' appears to give partitions which are reasonably},
	booktitle = {Proceedings of the {Fifth} {Berkeley} {Symposium} on {Mathematical} {Statistics} and {Probability}},
	publisher = {University of California Press},
	author = {MacQueen, J.},
	year = {1967},
	pages = {281--297},
	file = {Citeseer - Full Text PDF:/home/cisco/Zotero/storage/774Y45YA/Macqueen - 1967 - Some methods for classification and analysis of mu.pdf:application/pdf;Citeseer - Snapshot:/home/cisco/Zotero/storage/EIUCNY4H/summary.html:text/html}
}

@article{oord_representation_2019,
	title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
	url = {http://arxiv.org/abs/1807.03748},
	abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
	urldate = {2019-12-29},
	journal = {arXiv:1807.03748 [cs, stat]},
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	month = jan,
	year = {2019},
	note = {arXiv: 1807.03748},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/cisco/Zotero/storage/Y6VZYCEP/1807.html:text/html;arXiv Fulltext PDF:/home/cisco/Zotero/storage/NURFJCQR/Oord et al. - 2019 - Representation Learning with Contrastive Predictiv.pdf:application/pdf}
}

@article{kohout_network_2018,
	title = {Network {Traffic} {Fingerprinting} {Based} on {Approximated} {Kernel} {Two}-{Sample} {Test}},
	volume = {13},
	issn = {1556-6013},
	doi = {10.1109/TIFS.2017.2768018},
	abstract = {Many applications and communication protocols exhibit unique communication patterns that can be exploited to identify them in network traffic. This paper proposes a method to represent these patterns compactly, such that they can be used in different analytical tasks. The method treats each communication as a set of observations of a random variable with unknown probability distribution. This view allows us to derive the representation from a distance between two probability distributions used in maximum mean discrepancy-a non-parametric kernel test. The representation (and distance) can be then easily used in various algorithms for identification of communicating application and data analysis, independently of the specific type of input data.},
	number = {3},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Kohout, J. and Pevný, T.},
	month = mar,
	year = {2018},
	keywords = {analytical tasks, application identification, approximated kernel two-sample test, Communication fingerprinting, data analysis, Inspection, Kernel, Machine learning algorithms, maximum mean discrepancy, Memory management, network traffic fingerprinting, nonparametric kernel test, nonparametric statistics, Probability distribution, probability distributions, protocols, Protocols, Random variables, statistical distributions, telecommunication traffic, unique communication patterns},
	pages = {788--801},
	file = {IEEE Xplore Abstract Record:/home/cisco/Zotero/storage/JGIGN2GK/8089373.html:text/html}
}

@inproceedings{wang_solving_2000,
	address = {Stanford University, Stanford, CA, USA},
	title = {Solving {Multiple}-{Instance} {Problem}: {A} {Lazy} {Learning} {Approach}},
	shorttitle = {Solving {Multiple}-{Instance} {Problem}},
	url = {http://cogprints.org/2124/},
	abstract = {As opposed to traditional supervised learning, multiple-instance learning 
    concerns the problem of classifying a bag of instances, given bags that are 
    labeled by a teacher as being overall positive or negative. Current research 
    mainly concentrates on adapting traditional concept learning to solve this 
    problem. In this paper we investigate the use of lazy learning and Hausdorff 
    distance to approach the multiple-instance problem. We present two variants of 
    the K-nearest neighbor algorithm, called Bayesian-KNN and Citation-KNN, solving 
    the multiple-instance problem. Experiments on the Drug discovery benchmark data 
    show that both algorithms are competitive with the best ones conceived in the 
    concept learning framework. Further work includes exploring of a combination of 
    lazy and eager multiple-instance problem classifiers.},
	urldate = {2017-07-01},
	booktitle = {Proceedings of the {Seventeenth} {International} {Conference} on {Machine} {Learning}},
	publisher = {Morgan Kaufmann},
	author = {Wang, Jun and Zucker, Jean-Daniel},
	editor = {Langley, Pat},
	year = {2000},
	pages = {1119--1125}
}

@article{rippel_metric_2016,
	title = {Metric {Learning} with {Adaptive} {Density} {Discrimination}},
	url = {http://arxiv.org/abs/1511.05939},
	abstract = {Distance metric learning (DML) approaches learn a transformation to a representation space where distance is in correspondence with a predefined notion of similarity. While such models offer a number of compelling benefits, it has been difficult for these to compete with modern classification algorithms in performance and even in feature extraction. In this work, we propose a novel approach explicitly designed to address a number of subtle yet important issues which have stymied earlier DML algorithms. It maintains an explicit model of the distributions of the different classes in representation space. It then employs this knowledge to adaptively assess similarity, and achieve local discrimination by penalizing class distribution overlap. We demonstrate the effectiveness of this idea on several tasks. Our approach achieves state-of-the-art classification results on a number of fine-grained visual recognition datasets, surpassing the standard softmax classifier and outperforming triplet loss by a relative margin of 30-40\%. In terms of computational performance, it alleviates training inefficiencies in the traditional triplet loss, reaching the same error in 5-30 times fewer iterations. Beyond classification, we further validate the saliency of the learnt representations via their attribute concentration and hierarchy recovery properties, achieving 10-25\% relative gains on the softmax classifier and 25-50\% on triplet loss in these tasks.},
	urldate = {2020-06-05},
	journal = {arXiv:1511.05939 [cs, stat]},
	author = {Rippel, Oren and Paluri, Manohar and Dollar, Piotr and Bourdev, Lubomir},
	month = mar,
	year = {2016},
	note = {arXiv: 1511.05939},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	annote = {Comment: ICLR 2016},
	annote = {Comment: ICLR 2016}
}

@inproceedings{weinberger_distance_2006,
	title = {Distance {Metric} {Learning} for {Large} {Margin} {Nearest} {Neighbor} {Classification}},
	url = {http://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 18},
	publisher = {MIT Press},
	author = {Weinberger, Kilian Q and Blitzer, John and Saul, Lawrence K.},
	editor = {Weiss, Y. and Schölkopf, B. and Platt, J. C.},
	year = {2006},
	pages = {1473--1480}
}
